{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4ddd2fd-a069-476b-ac03-6c1002dc05d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "\n",
    "import openai\n",
    "from IPython.display import HTML, display\n",
    "from ipywidgets import widgets\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e16adde-8f33-4506-9bc8-a9ef2426f3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1072, which is longer than the specified 600\n",
      "Created a chunk of size 948, which is longer than the specified 600\n",
      "Created a chunk of size 973, which is longer than the specified 600\n",
      "Created a chunk of size 692, which is longer than the specified 600\n",
      "Created a chunk of size 1017, which is longer than the specified 600\n",
      "Created a chunk of size 733, which is longer than the specified 600\n",
      "Created a chunk of size 781, which is longer than the specified 600\n",
      "Created a chunk of size 1191, which is longer than the specified 600\n",
      "Created a chunk of size 630, which is longer than the specified 600\n",
      "Created a chunk of size 668, which is longer than the specified 600\n",
      "Created a chunk of size 679, which is longer than the specified 600\n",
      "Created a chunk of size 637, which is longer than the specified 600\n",
      "Created a chunk of size 766, which is longer than the specified 600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "482\n",
      "page_content=\"name: compare_values  parameters: a(The first value to compare.) b(The second value to compare.) \\ndescription: Compare the values of a and b. If a is less than b, return True; otherwise, return False. Note that values of different types cannot be compared. examples:compare_values(3, 5)  # returns True \\ncompare_values('abc', 'def')  # returns True \\ncompare_values(3, 'abc')  # raises an error\" metadata={}\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(        \n",
    "    separator = \"\\n\",\n",
    "    chunk_size = 600,\n",
    "    chunk_overlap = 100,\n",
    "    length_function = len,\n",
    ")\n",
    "\n",
    "with open('isrpa-api-full.txt') as f:\n",
    "    api_document = f.read()\n",
    "    \n",
    "docs = text_splitter.create_documents(api_document.split('\\n\\n'))\n",
    "print(len(docs))\n",
    "print(docs[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "069ee722-5664-4f4c-ba84-e78d3f21b571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "azure https://test0406.openai.azure.com/ 2023-03-15-preview\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "openai.api_type = os.getenv(\"OPENAI_API_TYPE\")\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai.api_base = os.getenv(\"OPENAI_API_BASE\")\n",
    "openai.api_version = os.getenv(\"OPENAI_API_VERSION\")\n",
    "print(openai.api_type, openai.api_base, openai.api_version)\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model='text-embedding-ada-002',\n",
    "                              deployment='emb0614',\n",
    "                              openai_api_base=os.getenv(\"OPENAI_API_BASE\"),\n",
    "                              openai_api_type=os.getenv(\"OPENAI_API_TYPE\"),\n",
    "                              chunk_size=1)\n",
    "\n",
    "# txt_docsearch = Chroma.from_documents(docs, embeddings, persist_directory='apidocs')\n",
    "# txt_docsearch.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27b92b5c-5475-4354-babd-817482b17109",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_docsearch = None\n",
    "txt_docsearch = Chroma(persist_directory='apidocs', embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56729be7-eddc-4af5-b5bd-aa4bb5def8a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! engine is not default parameter.\n",
      "                    engine was transferred to model_kwargs.\n",
      "                    Please confirm that engine is what you intended.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory, ConversationSummaryBufferMemory\n",
    "\n",
    "llm = ChatOpenAI(engine='test0406', model_name=\"gpt-3.5-turbo\", temperature=0.5, max_tokens=2048)\n",
    "\n",
    "prompt_q = \"\"\"根据聊天记录和任务描述，使用指定的函数来实现这个任务。\n",
    "聊天记录:\n",
    "{chat_history}\n",
    "任务: {question}\n",
    "生成代码:\"\"\"\n",
    "\n",
    "# 初始化 prompt 对象\n",
    "prompt = ChatPromptTemplate.from_template(prompt_q)\n",
    "# 初始化问答链\n",
    "question_generator = LLMChain(llm=llm, prompt=prompt)\n",
    "doc_chain = load_qa_chain(llm, chain_type=\"refine\")\n",
    "memory = ConversationSummaryBufferMemory(llm=llm,\n",
    "        max_token_limit=1000, memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "chain = ConversationalRetrievalChain(\n",
    "    retriever=txt_docsearch.as_retriever(),\n",
    "    question_generator=question_generator,\n",
    "    combine_docs_chain=doc_chain,\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "859b948e-8204-4db3-b3b6-ca716fd989d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': '操作步骤及说明\\n1.\\t业务人员配置好银行账号数据表，和邮箱账号数据；\\n2.\\t读取账号配置表，根据账号信息自动登录网银系统；\\n3.\\t获取前一天历史余额数据，写入资金日报表模板对应银行账户余额栏；\\n4.\\t下载前一天电子回单、网银流水，按照固定规则命名（命名规则：公司_收/付_账号_日期，例：X公司_收_账号_20211222），并在日志记录表中写入记录；\\n5.\\t将日报excel及回、流水文件夹进行压缩；\\n6.\\t邮件发送运行异常提醒，将运行的数据压缩文件发送到指定邮箱；\\n7.\\t业务人员接收邮件文件，进行确认操作。\\n', 'chat_history': [HumanMessage(content='操作步骤及说明\\n1.\\t业务人员配置好银行账号数据表，和邮箱账号数据；\\n2.\\t读取账号配置表，根据账号信息自动登录网银系统；\\n3.\\t获取前一天历史余额数据，写入资金日报表模板对应银行账户余额栏；\\n4.\\t下载前一天电子回单、网银流水，按照固定规则命名（命名规则：公司_收/付_账号_日期，例：X公司_收_账号_20211222），并在日志记录表中写入记录；\\n5.\\t将日报excel及回、流水文件夹进行压缩；\\n6.\\t邮件发送运行异常提醒，将运行的数据压缩文件发送到指定邮箱；\\n7.\\t业务人员接收邮件文件，进行确认操作。\\n', additional_kwargs={}, example=False), AIMessage(content='The new context provided does not relate to the original question, which asks for the steps to automate the daily financial report generation and email it to the relevant people. Therefore, the original answer remains the same.', additional_kwargs={}, example=False)], 'answer': 'The new context provided does not relate to the original question, which asks for the steps to automate the daily financial report generation and email it to the relevant people. Therefore, the original answer remains the same.'}\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"操作步骤及说明\n",
    "1.\t业务人员配置好银行账号数据表，和邮箱账号数据；\n",
    "2.\t读取账号配置表，根据账号信息自动登录网银系统；\n",
    "3.\t获取前一天历史余额数据，写入资金日报表模板对应银行账户余额栏；\n",
    "4.\t下载前一天电子回单、网银流水，按照固定规则命名（命名规则：公司_收/付_账号_日期，例：X公司_收_账号_20211222），并在日志记录表中写入记录；\n",
    "5.\t将日报excel及回、流水文件夹进行压缩；\n",
    "6.\t邮件发送运行异常提醒，将运行的数据压缩文件发送到指定邮箱；\n",
    "7.\t业务人员接收邮件文件，进行确认操作。\n",
    "\"\"\"\n",
    "result = chain({\"question\": query})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7fbe4b-71da-4e92-8aa8-f945f270cac3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
