{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "{}\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa1c4788-841f-4492-a630-7f5bed7e32c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b580c8b-3c26-4a8d-824a-543c6e64aefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "\n",
    "from InstructorEmbedding import INSTRUCTOR\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41cd0281-20f5-4bff-81ad-d73b6a360e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "docs_dir = Path('/home/jinzy/work/automation/privateGPT/source_documents')\n",
    "\n",
    "loader = DirectoryLoader(docs_dir, glob=\"./*.pdf\", loader_cls=PyPDFLoader)\n",
    "\n",
    "documents = loader.load()\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f16a9427-f388-4731-8e0a-5a90a37da48a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "320e8332-038a-4228-8f47-3e613336e3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "743\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "print(len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c496faec-7851-4f5c-852b-655b17343cd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bef44a9f75cd4c38ba4e879d02438722",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)7f436/.gitattributes:   0%|          | 0.00/1.48k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c25405b805f0448cb7e6b4c28ec82948",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)_Pooling/config.json:   0%|          | 0.00/270 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bad7adcab0847518a740081406b90eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/2_Dense/config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5cca40f9e48447cb1bd578f7fdac612",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/3.15M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b28c1117a86143a0bb0ddc3dcd6cd182",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)0daf57f436/README.md:   0%|          | 0.00/66.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b449b1032217424c926396ab14eb5643",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)af57f436/config.json:   0%|          | 0.00/1.52k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98eb79fd78394ee9b36e1ec4bb21fbb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ce_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d43a6315dbdf403dac70de2356b09897",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6d051815f134bb6b932d021209fac4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5270b214ccb640e6bc89680fb1fdd1b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11b411b5d33c4041a5d799fd57a3ac99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d854de858a94686b6adf613b58d3eec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)7f436/tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab56bbedc9ca4776b0e4ed29f61a40ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/2.40k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd7ddd75fc3f4575ae2608321ee78237",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)f57f436/modules.json:   0%|          | 0.00/461 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "\n",
    "instructor_embeddings = HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-xl\", \n",
    "                                                      model_kwargs={\"device\": \"cuda\"})\n",
    "# Embed and store the texts\n",
    "# Supplying a persist_directory will store the embeddings on disk\n",
    "persist_directory = 'db'\n",
    "vectordb = Chroma.from_documents(documents=texts, \n",
    "                                 embedding=instructor_embeddings,\n",
    "                                 persist_directory=persist_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f8aeeaa-fc75-42f4-97b6-aa5a063170a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# persiste the db to disk\n",
    "vectordb.persist()\n",
    "vectordb = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc48da40-c2e4-40c2-b261-aa799db715da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can load the persisted database from disk, and use it as normal. \n",
    "vectordb = Chroma(persist_directory=persist_directory, \n",
    "                  embedding_function=instructor_embeddings)\n",
    "retriever = vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3dfb0f2-e438-412c-9ff0-acad0c6d07d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Document(page_content='access.\\nWe propose FlashAttention , a new attention algorithm that computes exact attention with far fewer\\nmemory accesses. Our main goal is to avoid reading and writing the attention matrix to and from HBM.\\nThis requires (i) computing the softmax reduction without access to the whole input (ii) not storing the large\\nintermediate attention matrix for the backward pass. We apply two well-established techniques to address\\nthese challenges. (i) We restructure the attention computation to split the input into blocks and make several\\npasses over input blocks, thus incrementally performing the softmax reduction (also known as tiling). (ii) We\\nstore the softmax normalization factor from the forward pass to quickly recompute attention on-chip in the\\nbackward pass, which is faster than the standard approach of reading the intermediate attention matrix from\\nHBM. We implement FlashAttention in CUDA to achieve ﬁne-grained control over memory access and', metadata={'source': '/home/jinzy/work/automation/privateGPT/source_documents/Flash-attention.pdf', 'page': 1})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = retriever.get_relevant_documents(\"What is Flash attention?\")\n",
    "print(len(docs))\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bdb3da6-255f-48cf-841d-0cee808ba7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Document(page_content='access.\\nWe propose FlashAttention , a new attention algorithm that computes exact attention with far fewer\\nmemory accesses. Our main goal is to avoid reading and writing the attention matrix to and from HBM.\\nThis requires (i) computing the softmax reduction without access to the whole input (ii) not storing the large\\nintermediate attention matrix for the backward pass. We apply two well-established techniques to address\\nthese challenges. (i) We restructure the attention computation to split the input into blocks and make several\\npasses over input blocks, thus incrementally performing the softmax reduction (also known as tiling). (ii) We\\nstore the softmax normalization factor from the forward pass to quickly recompute attention on-chip in the\\nbackward pass, which is faster than the standard approach of reading the intermediate attention matrix from\\nHBM. We implement FlashAttention in CUDA to achieve ﬁne-grained control over memory access and', metadata={'source': '/home/jinzy/work/automation/privateGPT/source_documents/Flash-attention.pdf', 'page': 1})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = retriever.get_relevant_documents(\"请解释一下什么是flash attention?\")\n",
    "print(len(docs))\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872b67b2-44fe-4fff-b5cc-83404b970e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.embeddings.base import BaseEmbedding\n",
    "# from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "\n",
    "# class InstructEmbedding(BaseEmbedding):\n",
    "#     def __init__(self, handle: Optional[str] = None) -> None:\n",
    "#         \"\"\"Init params.\"\"\"\n",
    "#         try:\n",
    "#             self._instructor = HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-xl\", \n",
    "#                                                              model_kwargs={\"device\": \"cuda\"})\n",
    "#         except ImportError:\n",
    "#             raise ImportError(\n",
    "#                 \"Please install InstructorEmbeddings\"\n",
    "#             )\n",
    "\n",
    "#         super().__init__()\n",
    "\n",
    "#     def _get_query_embedding(self, query: str) -> List[float]:\n",
    "#         \"\"\"Get query embedding.\"\"\"\n",
    "#         return self._get_embedding(query)\n",
    "\n",
    "#     def _get_text_embedding(self, text: str) -> List[float]:\n",
    "#         \"\"\"Get text embedding.\"\"\"\n",
    "#         return self._get_embedding(text)\n",
    "\n",
    "#     def _get_embedding(self, text: str) -> List[float]:\n",
    "#         vectors = self._instruct([text]).numpy().tolist()\n",
    "#         return vectors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce702106-a6aa-4b7b-affe-74d6bb4181b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from llama_index import SimpleDirectoryReader, GPTVectorStoreIndex\n",
    "from llama_index import LangchainEmbedding, ServiceContext\n",
    "from llama_index.node_parser import SimpleNodeParser\n",
    "from llama_index.langchain_helpers.text_splitter import TokenTextSplitter\n",
    "\n",
    "\n",
    "node_parser = SimpleNodeParser(text_splitter=text_splitter)\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    embed_model=LangchainEmbedding(instructor_embeddings), node_parser=node_parser\n",
    ")\n",
    "\n",
    "documents = SimpleDirectoryReader(docs_dir, recursive=True, required_exts=['.txt', '.pdf']).load_data()\n",
    "index = GPTVectorStoreIndex.from_documents(documents, service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6b45478-03da-4dc4-b020-5c47436d3819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(response='\\nFlashAttention is an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. It is designed to reduce the amount of HBM access required for sequence length, compared to standard attention. FlashAttention also serves as a useful primitive for realizing the potential of approximate attention algorithms by overcoming their issues with memory access overhead.', source_nodes=[NodeWithScore(node=Node(text='in sequence length—than standard attention, thanks to the massively reduced amount of HBM access.\\nWe analyze the IO complexity [ 1] ofFlashAttention , proving that it requires 𝑂¹𝑁2𝑑2𝑀\\x001ºHBM\\naccesses where 𝑑is the head dimension and 𝑀is the size of SRAM, as compared to Ω¹𝑁𝑑¸𝑁2ºof standard\\nattention. For typical values of 𝑑and𝑀,FlashAttention requires many times fewer HBM accesses\\ncompared to standard attention (up to 9 \\x02fewer, as shown in Fig. 2). Moreover, we provide a lower bound,\\nshowing that no exact attention algorithm can asymptotically improve on the number of HBM accesses over\\nall SRAM sizes.\\nWe also show that FlashAttention can serve as a useful primitive for realizing the potential of\\napproximate attention algorithms by overcoming their issues with memory access overhead. As a proof of\\nconcept, we implement block-sparse FlashAttention , a sparse attention algorithm that is 2-4 \\x02faster than', doc_id='5b26abde-c087-4094-9289-00f8ee84de98', embedding=None, doc_hash='e7fecc06a3d028015c9f11aa98d17db69c31132ee52bef5654170c9a1c9245fa', extra_info=None, node_info=None, relationships={<DocumentRelationship.SOURCE: '1'>: '6d961bd7-7008-4f35-8c60-320380c8e02d', <DocumentRelationship.PREVIOUS: '2'>: 'dd6210e1-c793-4a7b-853c-1840150bccde', <DocumentRelationship.NEXT: '3'>: '3e3e5205-22d5-4837-aab6-c2526a257e00'}), score=0.7520940391060879), NodeWithScore(node=Node(text='aware—accounting for reads and writes between levels of GPU memory. We propose FlashAttention ,\\nan IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes\\nbetween GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity\\nofFlashAttention , showing that it requires fewer HBM accesses than standard attention, and is\\noptimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding\\nan approximate attention algorithm that is faster than any existing approximate attention method.\\nFlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup\\non BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3 \\x02speedup on\\nGPT-2 (seq. length 1K), and 2.4 \\x02speedup on long-range arena (seq. length 1K-4K). FlashAttention\\nand block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models', doc_id='47746a19-f796-4eb5-89c8-10e8714a8ad2', embedding=None, doc_hash='a465e3027bc51e3669161f4dc5bd294d615ca5df27756fe16769ef469c3e15e8', extra_info=None, node_info=None, relationships={<DocumentRelationship.SOURCE: '1'>: '6d961bd7-7008-4f35-8c60-320380c8e02d', <DocumentRelationship.PREVIOUS: '2'>: 'e1eaef1f-02db-4f07-a9be-099fa085975a', <DocumentRelationship.NEXT: '3'>: 'd2f85d1b-bb4d-4bfb-95d6-29bd6325cc10'}), score=0.7482484138637523)], extra_info={'5b26abde-c087-4094-9289-00f8ee84de98': None, '47746a19-f796-4eb5-89c8-10e8714a8ad2': None})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "query_engine.query('请解释一下什么是flash attention?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8e290f4-1282-4798-8b85-5e7f9cd6db19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(response='\\nFlash Attention是一种IO感知的精确注意力算法，它使用平铺来减少GPU高带宽存储器（HBM）和GPU片上SRAM之间的读/写次数。它比标准注意力更有效地利用HBM访问，并且可以提供更长的序列长度。', source_nodes=[NodeWithScore(node=Node(text='in sequence length—than standard attention, thanks to the massively reduced amount of HBM access.\\nWe analyze the IO complexity [ 1] ofFlashAttention , proving that it requires 𝑂¹𝑁2𝑑2𝑀\\x001ºHBM\\naccesses where 𝑑is the head dimension and 𝑀is the size of SRAM, as compared to Ω¹𝑁𝑑¸𝑁2ºof standard\\nattention. For typical values of 𝑑and𝑀,FlashAttention requires many times fewer HBM accesses\\ncompared to standard attention (up to 9 \\x02fewer, as shown in Fig. 2). Moreover, we provide a lower bound,\\nshowing that no exact attention algorithm can asymptotically improve on the number of HBM accesses over\\nall SRAM sizes.\\nWe also show that FlashAttention can serve as a useful primitive for realizing the potential of\\napproximate attention algorithms by overcoming their issues with memory access overhead. As a proof of\\nconcept, we implement block-sparse FlashAttention , a sparse attention algorithm that is 2-4 \\x02faster than', doc_id='5b26abde-c087-4094-9289-00f8ee84de98', embedding=None, doc_hash='e7fecc06a3d028015c9f11aa98d17db69c31132ee52bef5654170c9a1c9245fa', extra_info=None, node_info=None, relationships={<DocumentRelationship.SOURCE: '1'>: '6d961bd7-7008-4f35-8c60-320380c8e02d', <DocumentRelationship.PREVIOUS: '2'>: 'dd6210e1-c793-4a7b-853c-1840150bccde', <DocumentRelationship.NEXT: '3'>: '3e3e5205-22d5-4837-aab6-c2526a257e00'}), score=0.7520940391060879), NodeWithScore(node=Node(text='aware—accounting for reads and writes between levels of GPU memory. We propose FlashAttention ,\\nan IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes\\nbetween GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity\\nofFlashAttention , showing that it requires fewer HBM accesses than standard attention, and is\\noptimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding\\nan approximate attention algorithm that is faster than any existing approximate attention method.\\nFlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup\\non BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3 \\x02speedup on\\nGPT-2 (seq. length 1K), and 2.4 \\x02speedup on long-range arena (seq. length 1K-4K). FlashAttention\\nand block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models', doc_id='47746a19-f796-4eb5-89c8-10e8714a8ad2', embedding=None, doc_hash='a465e3027bc51e3669161f4dc5bd294d615ca5df27756fe16769ef469c3e15e8', extra_info=None, node_info=None, relationships={<DocumentRelationship.SOURCE: '1'>: '6d961bd7-7008-4f35-8c60-320380c8e02d', <DocumentRelationship.PREVIOUS: '2'>: 'e1eaef1f-02db-4f07-a9be-099fa085975a', <DocumentRelationship.NEXT: '3'>: 'd2f85d1b-bb4d-4bfb-95d6-29bd6325cc10'}), score=0.7482484138637523)], extra_info={'5b26abde-c087-4094-9289-00f8ee84de98': None, '47746a19-f796-4eb5-89c8-10e8714a8ad2': None})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_engine.query('请用中文解释一下什么是flash attention?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4dd79f57-0d70-433d-962a-4be71cabb2c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(response='\\nFlash Attention是一种深度学习操作，它可以帮助计算机快速地理解文本中的关系。它可以比标准注意力更快地处理更长的序列，并且可以减少计算机需要访问的内存，从而提高计算机的性能。', source_nodes=[NodeWithScore(node=Node(text='in sequence length—than standard attention, thanks to the massively reduced amount of HBM access.\\nWe analyze the IO complexity [ 1] ofFlashAttention , proving that it requires 𝑂¹𝑁2𝑑2𝑀\\x001ºHBM\\naccesses where 𝑑is the head dimension and 𝑀is the size of SRAM, as compared to Ω¹𝑁𝑑¸𝑁2ºof standard\\nattention. For typical values of 𝑑and𝑀,FlashAttention requires many times fewer HBM accesses\\ncompared to standard attention (up to 9 \\x02fewer, as shown in Fig. 2). Moreover, we provide a lower bound,\\nshowing that no exact attention algorithm can asymptotically improve on the number of HBM accesses over\\nall SRAM sizes.\\nWe also show that FlashAttention can serve as a useful primitive for realizing the potential of\\napproximate attention algorithms by overcoming their issues with memory access overhead. As a proof of\\nconcept, we implement block-sparse FlashAttention , a sparse attention algorithm that is 2-4 \\x02faster than', doc_id='5b26abde-c087-4094-9289-00f8ee84de98', embedding=None, doc_hash='e7fecc06a3d028015c9f11aa98d17db69c31132ee52bef5654170c9a1c9245fa', extra_info=None, node_info=None, relationships={<DocumentRelationship.SOURCE: '1'>: '6d961bd7-7008-4f35-8c60-320380c8e02d', <DocumentRelationship.PREVIOUS: '2'>: 'dd6210e1-c793-4a7b-853c-1840150bccde', <DocumentRelationship.NEXT: '3'>: '3e3e5205-22d5-4837-aab6-c2526a257e00'}), score=0.7384288130652585), NodeWithScore(node=Node(text='to scale to even longer sequences (64K), resulting in the ﬁrst model that can achieve better-than-chance\\nperformance on Path-256.\\n•Benchmarking Attention. FlashAttention is up to 3\\x02faster than the standard attention implemen-\\ntation across common sequence lengths from 128 to 2K and scales up to 64K. Up to sequence length of 512,\\nFlashAttention is both faster and more memory-eﬃcient than any existing attention method, whereas\\nfor sequence length beyond 1K, some approximate attention methods (e.g., Linformer) start to become\\nfaster. On the other hand, block-sparse FlashAttention is faster than all existing approximate attention\\nmethods that we know of.\\n2 Background\\nWe provide some background on the performance characteristics of common deep learning operations on\\nmodern hardware (GPUs). We also describe the standard implementation of attention.\\n2.1 Hardware Performance\\nWe focus here on GPUs. Performance on other hardware accelerators are similar [46, 48].', doc_id='89743d4a-039e-400a-90c2-00df9e094e8f', embedding=None, doc_hash='12731e3ecd68259c77a12849138ec5b646185a67e7551ae440d4e806c75921c2', extra_info=None, node_info=None, relationships={<DocumentRelationship.SOURCE: '1'>: '6d961bd7-7008-4f35-8c60-320380c8e02d', <DocumentRelationship.PREVIOUS: '2'>: '34fb509a-1094-46c4-aa57-5869e36289fc', <DocumentRelationship.NEXT: '3'>: '4a2f3200-cf63-4057-9b89-2dc1b360df40'}), score=0.7346827136193597)], extra_info={'5b26abde-c087-4094-9289-00f8ee84de98': None, '89743d4a-039e-400a-90c2-00df9e094e8f': None})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_engine.query('请用中文解释一下什么是flash attention?假设我是5岁小孩，尽可能直观、详细')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894e778f-e222-47cb-824b-d2f65db9a164",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
