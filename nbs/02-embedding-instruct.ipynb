{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "{}\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa1c4788-841f-4492-a630-7f5bed7e32c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b580c8b-3c26-4a8d-824a-543c6e64aefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "\n",
    "from InstructorEmbedding import INSTRUCTOR\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41cd0281-20f5-4bff-81ad-d73b6a360e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "docs_dir = Path('/home/jinzy/work/automation/privateGPT/source_documents')\n",
    "\n",
    "loader = DirectoryLoader(docs_dir, glob=\"./*.pdf\", loader_cls=PyPDFLoader)\n",
    "\n",
    "documents = loader.load()\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f16a9427-f388-4731-8e0a-5a90a37da48a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "320e8332-038a-4228-8f47-3e613336e3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "743\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "print(len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c496faec-7851-4f5c-852b-655b17343cd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bef44a9f75cd4c38ba4e879d02438722",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)7f436/.gitattributes:   0%|          | 0.00/1.48k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c25405b805f0448cb7e6b4c28ec82948",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)_Pooling/config.json:   0%|          | 0.00/270 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bad7adcab0847518a740081406b90eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)/2_Dense/config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5cca40f9e48447cb1bd578f7fdac612",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/3.15M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b28c1117a86143a0bb0ddc3dcd6cd182",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)0daf57f436/README.md:   0%|          | 0.00/66.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b449b1032217424c926396ab14eb5643",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)af57f436/config.json:   0%|          | 0.00/1.52k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98eb79fd78394ee9b36e1ec4bb21fbb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)ce_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d43a6315dbdf403dac70de2356b09897",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6d051815f134bb6b932d021209fac4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5270b214ccb640e6bc89680fb1fdd1b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)cial_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11b411b5d33c4041a5d799fd57a3ac99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d854de858a94686b6adf613b58d3eec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)7f436/tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab56bbedc9ca4776b0e4ed29f61a40ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)okenizer_config.json:   0%|          | 0.00/2.40k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd7ddd75fc3f4575ae2608321ee78237",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)f57f436/modules.json:   0%|          | 0.00/461 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "\n",
    "instructor_embeddings = HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-xl\", \n",
    "                                                      model_kwargs={\"device\": \"cuda\"})\n",
    "# Embed and store the texts\n",
    "# Supplying a persist_directory will store the embeddings on disk\n",
    "persist_directory = 'db'\n",
    "vectordb = Chroma.from_documents(documents=texts, \n",
    "                                 embedding=instructor_embeddings,\n",
    "                                 persist_directory=persist_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f8aeeaa-fc75-42f4-97b6-aa5a063170a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# persiste the db to disk\n",
    "vectordb.persist()\n",
    "vectordb = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc48da40-c2e4-40c2-b261-aa799db715da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can load the persisted database from disk, and use it as normal. \n",
    "vectordb = Chroma(persist_directory=persist_directory, \n",
    "                  embedding_function=instructor_embeddings)\n",
    "retriever = vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3dfb0f2-e438-412c-9ff0-acad0c6d07d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Document(page_content='access.\\nWe propose FlashAttention , a new attention algorithm that computes exact attention with far fewer\\nmemory accesses. Our main goal is to avoid reading and writing the attention matrix to and from HBM.\\nThis requires (i) computing the softmax reduction without access to the whole input (ii) not storing the large\\nintermediate attention matrix for the backward pass. We apply two well-established techniques to address\\nthese challenges. (i) We restructure the attention computation to split the input into blocks and make several\\npasses over input blocks, thus incrementally performing the softmax reduction (also known as tiling). (ii) We\\nstore the softmax normalization factor from the forward pass to quickly recompute attention on-chip in the\\nbackward pass, which is faster than the standard approach of reading the intermediate attention matrix from\\nHBM. We implement FlashAttention in CUDA to achieve ï¬ne-grained control over memory access and', metadata={'source': '/home/jinzy/work/automation/privateGPT/source_documents/Flash-attention.pdf', 'page': 1})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = retriever.get_relevant_documents(\"What is Flash attention?\")\n",
    "print(len(docs))\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bdb3da6-255f-48cf-841d-0cee808ba7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Document(page_content='access.\\nWe propose FlashAttention , a new attention algorithm that computes exact attention with far fewer\\nmemory accesses. Our main goal is to avoid reading and writing the attention matrix to and from HBM.\\nThis requires (i) computing the softmax reduction without access to the whole input (ii) not storing the large\\nintermediate attention matrix for the backward pass. We apply two well-established techniques to address\\nthese challenges. (i) We restructure the attention computation to split the input into blocks and make several\\npasses over input blocks, thus incrementally performing the softmax reduction (also known as tiling). (ii) We\\nstore the softmax normalization factor from the forward pass to quickly recompute attention on-chip in the\\nbackward pass, which is faster than the standard approach of reading the intermediate attention matrix from\\nHBM. We implement FlashAttention in CUDA to achieve ï¬ne-grained control over memory access and', metadata={'source': '/home/jinzy/work/automation/privateGPT/source_documents/Flash-attention.pdf', 'page': 1})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = retriever.get_relevant_documents(\"è¯·è§£é‡Šä¸€ä¸‹ä»€ä¹ˆæ˜¯flash attention?\")\n",
    "print(len(docs))\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872b67b2-44fe-4fff-b5cc-83404b970e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.embeddings.base import BaseEmbedding\n",
    "# from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "\n",
    "# class InstructEmbedding(BaseEmbedding):\n",
    "#     def __init__(self, handle: Optional[str] = None) -> None:\n",
    "#         \"\"\"Init params.\"\"\"\n",
    "#         try:\n",
    "#             self._instructor = HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-xl\", \n",
    "#                                                              model_kwargs={\"device\": \"cuda\"})\n",
    "#         except ImportError:\n",
    "#             raise ImportError(\n",
    "#                 \"Please install InstructorEmbeddings\"\n",
    "#             )\n",
    "\n",
    "#         super().__init__()\n",
    "\n",
    "#     def _get_query_embedding(self, query: str) -> List[float]:\n",
    "#         \"\"\"Get query embedding.\"\"\"\n",
    "#         return self._get_embedding(query)\n",
    "\n",
    "#     def _get_text_embedding(self, text: str) -> List[float]:\n",
    "#         \"\"\"Get text embedding.\"\"\"\n",
    "#         return self._get_embedding(text)\n",
    "\n",
    "#     def _get_embedding(self, text: str) -> List[float]:\n",
    "#         vectors = self._instruct([text]).numpy().tolist()\n",
    "#         return vectors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce702106-a6aa-4b7b-affe-74d6bb4181b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from llama_index import SimpleDirectoryReader, GPTVectorStoreIndex\n",
    "from llama_index import LangchainEmbedding, ServiceContext\n",
    "from llama_index.node_parser import SimpleNodeParser\n",
    "from llama_index.langchain_helpers.text_splitter import TokenTextSplitter\n",
    "\n",
    "\n",
    "node_parser = SimpleNodeParser(text_splitter=text_splitter)\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    embed_model=LangchainEmbedding(instructor_embeddings), node_parser=node_parser\n",
    ")\n",
    "\n",
    "documents = SimpleDirectoryReader(docs_dir, recursive=True, required_exts=['.txt', '.pdf']).load_data()\n",
    "index = GPTVectorStoreIndex.from_documents(documents, service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6b45478-03da-4dc4-b020-5c47436d3819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(response='\\nFlashAttention is an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. It is designed to reduce the amount of HBM access required for sequence length, compared to standard attention. FlashAttention also serves as a useful primitive for realizing the potential of approximate attention algorithms by overcoming their issues with memory access overhead.', source_nodes=[NodeWithScore(node=Node(text='in sequence lengthâ€”than standard attention, thanks to the massively reduced amount of HBM access.\\nWe analyze the IO complexity [ 1] ofFlashAttention , proving that it requires ğ‘‚Â¹ğ‘2ğ‘‘2ğ‘€\\x001ÂºHBM\\naccesses where ğ‘‘is the head dimension and ğ‘€is the size of SRAM, as compared to Î©Â¹ğ‘ğ‘‘Â¸ğ‘2Âºof standard\\nattention. For typical values of ğ‘‘andğ‘€,FlashAttention requires many times fewer HBM accesses\\ncompared to standard attention (up to 9 \\x02fewer, as shown in Fig. 2). Moreover, we provide a lower bound,\\nshowing that no exact attention algorithm can asymptotically improve on the number of HBM accesses over\\nall SRAM sizes.\\nWe also show that FlashAttention can serve as a useful primitive for realizing the potential of\\napproximate attention algorithms by overcoming their issues with memory access overhead. As a proof of\\nconcept, we implement block-sparse FlashAttention , a sparse attention algorithm that is 2-4 \\x02faster than', doc_id='5b26abde-c087-4094-9289-00f8ee84de98', embedding=None, doc_hash='e7fecc06a3d028015c9f11aa98d17db69c31132ee52bef5654170c9a1c9245fa', extra_info=None, node_info=None, relationships={<DocumentRelationship.SOURCE: '1'>: '6d961bd7-7008-4f35-8c60-320380c8e02d', <DocumentRelationship.PREVIOUS: '2'>: 'dd6210e1-c793-4a7b-853c-1840150bccde', <DocumentRelationship.NEXT: '3'>: '3e3e5205-22d5-4837-aab6-c2526a257e00'}), score=0.7520940391060879), NodeWithScore(node=Node(text='awareâ€”accounting for reads and writes between levels of GPU memory. We propose FlashAttention ,\\nan IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes\\nbetween GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity\\nofFlashAttention , showing that it requires fewer HBM accesses than standard attention, and is\\noptimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding\\nan approximate attention algorithm that is faster than any existing approximate attention method.\\nFlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup\\non BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3 \\x02speedup on\\nGPT-2 (seq. length 1K), and 2.4 \\x02speedup on long-range arena (seq. length 1K-4K). FlashAttention\\nand block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models', doc_id='47746a19-f796-4eb5-89c8-10e8714a8ad2', embedding=None, doc_hash='a465e3027bc51e3669161f4dc5bd294d615ca5df27756fe16769ef469c3e15e8', extra_info=None, node_info=None, relationships={<DocumentRelationship.SOURCE: '1'>: '6d961bd7-7008-4f35-8c60-320380c8e02d', <DocumentRelationship.PREVIOUS: '2'>: 'e1eaef1f-02db-4f07-a9be-099fa085975a', <DocumentRelationship.NEXT: '3'>: 'd2f85d1b-bb4d-4bfb-95d6-29bd6325cc10'}), score=0.7482484138637523)], extra_info={'5b26abde-c087-4094-9289-00f8ee84de98': None, '47746a19-f796-4eb5-89c8-10e8714a8ad2': None})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "query_engine.query('è¯·è§£é‡Šä¸€ä¸‹ä»€ä¹ˆæ˜¯flash attention?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8e290f4-1282-4798-8b85-5e7f9cd6db19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(response='\\nFlash Attentionæ˜¯ä¸€ç§IOæ„ŸçŸ¥çš„ç²¾ç¡®æ³¨æ„åŠ›ç®—æ³•ï¼Œå®ƒä½¿ç”¨å¹³é“ºæ¥å‡å°‘GPUé«˜å¸¦å®½å­˜å‚¨å™¨ï¼ˆHBMï¼‰å’ŒGPUç‰‡ä¸ŠSRAMä¹‹é—´çš„è¯»/å†™æ¬¡æ•°ã€‚å®ƒæ¯”æ ‡å‡†æ³¨æ„åŠ›æ›´æœ‰æ•ˆåœ°åˆ©ç”¨HBMè®¿é—®ï¼Œå¹¶ä¸”å¯ä»¥æä¾›æ›´é•¿çš„åºåˆ—é•¿åº¦ã€‚', source_nodes=[NodeWithScore(node=Node(text='in sequence lengthâ€”than standard attention, thanks to the massively reduced amount of HBM access.\\nWe analyze the IO complexity [ 1] ofFlashAttention , proving that it requires ğ‘‚Â¹ğ‘2ğ‘‘2ğ‘€\\x001ÂºHBM\\naccesses where ğ‘‘is the head dimension and ğ‘€is the size of SRAM, as compared to Î©Â¹ğ‘ğ‘‘Â¸ğ‘2Âºof standard\\nattention. For typical values of ğ‘‘andğ‘€,FlashAttention requires many times fewer HBM accesses\\ncompared to standard attention (up to 9 \\x02fewer, as shown in Fig. 2). Moreover, we provide a lower bound,\\nshowing that no exact attention algorithm can asymptotically improve on the number of HBM accesses over\\nall SRAM sizes.\\nWe also show that FlashAttention can serve as a useful primitive for realizing the potential of\\napproximate attention algorithms by overcoming their issues with memory access overhead. As a proof of\\nconcept, we implement block-sparse FlashAttention , a sparse attention algorithm that is 2-4 \\x02faster than', doc_id='5b26abde-c087-4094-9289-00f8ee84de98', embedding=None, doc_hash='e7fecc06a3d028015c9f11aa98d17db69c31132ee52bef5654170c9a1c9245fa', extra_info=None, node_info=None, relationships={<DocumentRelationship.SOURCE: '1'>: '6d961bd7-7008-4f35-8c60-320380c8e02d', <DocumentRelationship.PREVIOUS: '2'>: 'dd6210e1-c793-4a7b-853c-1840150bccde', <DocumentRelationship.NEXT: '3'>: '3e3e5205-22d5-4837-aab6-c2526a257e00'}), score=0.7520940391060879), NodeWithScore(node=Node(text='awareâ€”accounting for reads and writes between levels of GPU memory. We propose FlashAttention ,\\nan IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes\\nbetween GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity\\nofFlashAttention , showing that it requires fewer HBM accesses than standard attention, and is\\noptimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding\\nan approximate attention algorithm that is faster than any existing approximate attention method.\\nFlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup\\non BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3 \\x02speedup on\\nGPT-2 (seq. length 1K), and 2.4 \\x02speedup on long-range arena (seq. length 1K-4K). FlashAttention\\nand block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models', doc_id='47746a19-f796-4eb5-89c8-10e8714a8ad2', embedding=None, doc_hash='a465e3027bc51e3669161f4dc5bd294d615ca5df27756fe16769ef469c3e15e8', extra_info=None, node_info=None, relationships={<DocumentRelationship.SOURCE: '1'>: '6d961bd7-7008-4f35-8c60-320380c8e02d', <DocumentRelationship.PREVIOUS: '2'>: 'e1eaef1f-02db-4f07-a9be-099fa085975a', <DocumentRelationship.NEXT: '3'>: 'd2f85d1b-bb4d-4bfb-95d6-29bd6325cc10'}), score=0.7482484138637523)], extra_info={'5b26abde-c087-4094-9289-00f8ee84de98': None, '47746a19-f796-4eb5-89c8-10e8714a8ad2': None})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_engine.query('è¯·ç”¨ä¸­æ–‡è§£é‡Šä¸€ä¸‹ä»€ä¹ˆæ˜¯flash attention?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4dd79f57-0d70-433d-962a-4be71cabb2c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(response='\\nFlash Attentionæ˜¯ä¸€ç§æ·±åº¦å­¦ä¹ æ“ä½œï¼Œå®ƒå¯ä»¥å¸®åŠ©è®¡ç®—æœºå¿«é€Ÿåœ°ç†è§£æ–‡æœ¬ä¸­çš„å…³ç³»ã€‚å®ƒå¯ä»¥æ¯”æ ‡å‡†æ³¨æ„åŠ›æ›´å¿«åœ°å¤„ç†æ›´é•¿çš„åºåˆ—ï¼Œå¹¶ä¸”å¯ä»¥å‡å°‘è®¡ç®—æœºéœ€è¦è®¿é—®çš„å†…å­˜ï¼Œä»è€Œæé«˜è®¡ç®—æœºçš„æ€§èƒ½ã€‚', source_nodes=[NodeWithScore(node=Node(text='in sequence lengthâ€”than standard attention, thanks to the massively reduced amount of HBM access.\\nWe analyze the IO complexity [ 1] ofFlashAttention , proving that it requires ğ‘‚Â¹ğ‘2ğ‘‘2ğ‘€\\x001ÂºHBM\\naccesses where ğ‘‘is the head dimension and ğ‘€is the size of SRAM, as compared to Î©Â¹ğ‘ğ‘‘Â¸ğ‘2Âºof standard\\nattention. For typical values of ğ‘‘andğ‘€,FlashAttention requires many times fewer HBM accesses\\ncompared to standard attention (up to 9 \\x02fewer, as shown in Fig. 2). Moreover, we provide a lower bound,\\nshowing that no exact attention algorithm can asymptotically improve on the number of HBM accesses over\\nall SRAM sizes.\\nWe also show that FlashAttention can serve as a useful primitive for realizing the potential of\\napproximate attention algorithms by overcoming their issues with memory access overhead. As a proof of\\nconcept, we implement block-sparse FlashAttention , a sparse attention algorithm that is 2-4 \\x02faster than', doc_id='5b26abde-c087-4094-9289-00f8ee84de98', embedding=None, doc_hash='e7fecc06a3d028015c9f11aa98d17db69c31132ee52bef5654170c9a1c9245fa', extra_info=None, node_info=None, relationships={<DocumentRelationship.SOURCE: '1'>: '6d961bd7-7008-4f35-8c60-320380c8e02d', <DocumentRelationship.PREVIOUS: '2'>: 'dd6210e1-c793-4a7b-853c-1840150bccde', <DocumentRelationship.NEXT: '3'>: '3e3e5205-22d5-4837-aab6-c2526a257e00'}), score=0.7384288130652585), NodeWithScore(node=Node(text='to scale to even longer sequences (64K), resulting in the ï¬rst model that can achieve better-than-chance\\nperformance on Path-256.\\nâ€¢Benchmarking Attention. FlashAttention is up to 3\\x02faster than the standard attention implemen-\\ntation across common sequence lengths from 128 to 2K and scales up to 64K. Up to sequence length of 512,\\nFlashAttention is both faster and more memory-eï¬ƒcient than any existing attention method, whereas\\nfor sequence length beyond 1K, some approximate attention methods (e.g., Linformer) start to become\\nfaster. On the other hand, block-sparse FlashAttention is faster than all existing approximate attention\\nmethods that we know of.\\n2 Background\\nWe provide some background on the performance characteristics of common deep learning operations on\\nmodern hardware (GPUs). We also describe the standard implementation of attention.\\n2.1 Hardware Performance\\nWe focus here on GPUs. Performance on other hardware accelerators are similar [46, 48].', doc_id='89743d4a-039e-400a-90c2-00df9e094e8f', embedding=None, doc_hash='12731e3ecd68259c77a12849138ec5b646185a67e7551ae440d4e806c75921c2', extra_info=None, node_info=None, relationships={<DocumentRelationship.SOURCE: '1'>: '6d961bd7-7008-4f35-8c60-320380c8e02d', <DocumentRelationship.PREVIOUS: '2'>: '34fb509a-1094-46c4-aa57-5869e36289fc', <DocumentRelationship.NEXT: '3'>: '4a2f3200-cf63-4057-9b89-2dc1b360df40'}), score=0.7346827136193597)], extra_info={'5b26abde-c087-4094-9289-00f8ee84de98': None, '89743d4a-039e-400a-90c2-00df9e094e8f': None})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_engine.query('è¯·ç”¨ä¸­æ–‡è§£é‡Šä¸€ä¸‹ä»€ä¹ˆæ˜¯flash attention?å‡è®¾æˆ‘æ˜¯5å²å°å­©ï¼Œå°½å¯èƒ½ç›´è§‚ã€è¯¦ç»†')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894e778f-e222-47cb-824b-d2f65db9a164",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
