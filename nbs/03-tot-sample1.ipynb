{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "{}\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "813f2cb9-e0a1-4d4f-b3d9-1d99b8bb4dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai.api_type = os.getenv(\"OPENAI_API_TYPE\")\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai.api_base = os.getenv(\"OPENAI_DEPLOYMENT_ENDPOINT\")\n",
    "openai.api_version = os.getenv(\"OPENAI_DEPLOYMENT_VERSION\")\n",
    "\n",
    "engine_name = os.getenv(\"OPENAI_DEPLOYMENT_NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81fc2a95-bd9c-4d1f-854a-8ba09eed19ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Note: The openai-python library support for Azure OpenAI is in preview.\n",
    "# import os\n",
    "# import openai\n",
    "# openai.api_type = \"azure\"\n",
    "# openai.api_base = \"https://auto80p.openai.azure.com/\"\n",
    "# openai.api_version = \"2023-03-15-preview\"\n",
    "# openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# response = openai.ChatCompletion.create(\n",
    "#   engine=\"gpt35\",\n",
    "#   messages = [],\n",
    "#   temperature=0.7,\n",
    "#   max_tokens=800,\n",
    "#   top_p=0.95,\n",
    "#   frequency_penalty=0,\n",
    "#   presence_penalty=0,\n",
    "#   stop=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39fbcc2f-a246-4e91-8b7f-ec0b266a7b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! engine is not default parameter.\n",
      "                    engine was transferred to model_kwargs.\n",
      "                    Please confirm that engine is what you intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: Carlos likely forgot his watch when he left it at the snack bar. He may have left it on the lounger or dropped it on the way to the snack bar, but it's most likely in the towel he left at the snack bar.\n",
      "B: I agree that the most likely location of the watch is at the snack bar because he left the towel there after shaking it out. It could have been in the towel when he shook it out and it fell out or it could have been left in the towel. It's also possible that he lost it on the way to the snack bar or at the diving board, but we don't have any evidence to support that.\n",
      "C: I also agree that the most likely location of the watch is at the snack bar. While it's possible that he dropped it on the way to the snack bar or at the diving board, the fact that he put the watch in the towel and then left the towel at the snack bar makes it the most likely location. It's also possible that the watch fell out of the towel when he shook it out, but we don't have any evidence to support that. \n",
      "A: Another possibility is that Carlos put the watch in his pocket after shaking the towel and forgot about it. But given that he put the watch in the towel and then left the towel at the snack bar, it's more likely that he left it in the towel. \n",
      "B: That's true, but we don't have any evidence to support the idea that he put the watch in his pocket after shaking the towel. It's possible, but less likely. Based on the evidence we have, I still think the snack bar is the most likely location.\n",
      "C: I agree that the snack bar is still the most likely location. While it's possible that he put the watch in his pocket, we don't have any evidence to support that. \n",
      "A: Agreed. So we all agree that the most likely location of the watch is at the snack bar. \n",
      "B: Yes, I think that's the most reasonable conclusion based on the evidence we have.\n",
      "C: I agree. It's also possible that the watch fell out of the towel when he shook it out, but given that he left the towel at the snack bar, that's the most likely location. \n",
      "A: So we all agree that the most likely location of the watch is at the snack bar. \n",
      "B: Yes, I think that's the most reasonable conclusion based on the evidence we have.\n",
      "C: I agree. It's also possible that the watch fell out of the towel when he shook it out, but given that he left the towel at the snack bar, that's the most likely location. \n",
      "All experts agree that the single most likely location of Carlos' watch is at the snack bar. \n",
      "\n",
      "The experts rate the likelihood of their conclusion being correct...\n",
      "A: I rate my conclusion that the watch is at the snack bar as 85% likely to be correct.\n",
      "B: I agree, I would also rate our conclusion as 85% likely to be correct.\n",
      "C: Yes, I agree. Our conclusion is based on the most plausible scenario given the evidence we have, so I would also rate it as 85% likely to be correct. \n",
      "Overall, the experts conclude that Carlos' watch is most likely at the snack bar with an 85% likelihood of being correct. They note that it's still possible that the watch fell out of the towel when Carlos shook it out, but based on the evidence provided, the snack bar is the most likely location. They also acknowledge that if additional information becomes available, their conclusion may change. \n",
      "<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "llm_1 = OpenAI(engine=engine_name, temperature=0.7, max_tokens=3000)\n",
    "\n",
    "template = \"\"\"Imagine three different experts are answering this question.\n",
    "They will brainstorm the answer step by step reasoning carefully and taking all facts into consideration\n",
    "All experts will write down 1 step of their thinking,\n",
    "then share it with the group.\n",
    "They will each critique their response, and the all the responses of others\n",
    "They will check their answer based on science and the laws of physics\n",
    "Then all experts will go on to the next step and write down this step of their thinking.\n",
    "They will keep going through steps until they reach their conclusion taking into account the thoughts of the other experts\n",
    "If at any time they realise that there is a flaw in their logic they will backtrack to where that flaw occurred \n",
    "If any expert realises they're wrong at any point then they acknowledges this and start another train of thought\n",
    "Each expert will assign a likelihood of their current assertion being correct\n",
    "Continue until the experts agree on the single most likely location\n",
    "The question is {question}\n",
    "\n",
    "The experts reasoning is...\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "question = \"\"\"1. Carlos is at the swimming pool. \n",
    "2. He walks to the locker room, carrying a towel. \n",
    "3. He puts his watch in the towel and carries the towel tightly to a lounger at the poolside. \n",
    "4. At the lounger he opens and vigorously shakes the towel, then walks to the snack bar. \n",
    "5. He leaves the towel at the snack bar, then walks to the diving board. \n",
    "6. Later Carlos realises he has has lost his watch. Where is the single most likely location of the watch? \"\"\"\n",
    "\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm_1)\n",
    "\n",
    "response_1 = llm_chain.run(question)\n",
    "\n",
    "print(response_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bc9919-f1cc-4ce1-bc5b-4bffdbd53f01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
