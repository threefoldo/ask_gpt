{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "{}\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a50cb09c-8ac6-458e-9fff-1cba9a0ca45c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "\n",
    "import openai\n",
    "from IPython.display import HTML, display\n",
    "from ipywidgets import widgets\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c97be6ae-40ab-4bfa-9bcf-e62da6972a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 224/224 [14:08<00:00,  3.79s/it]\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "loader = DirectoryLoader(\"/home/jinzy/work/automation/ask_gpt/documents\", glob=\"**/*.pdf\", show_progress=True, use_multithreading=True)\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size = 400,\n",
    "    chunk_overlap  = 30,\n",
    "    length_function = len,\n",
    ")\n",
    "txt_docs = loader.load_and_split(text_splitter=text_splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efec7d9c-c167-4961-bc21-d82fa1272421",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised APIError: The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 3c4b3dc691296b6e6980815c4b48ddd2 in your message.) {\n",
      "  \"error\": {\n",
      "    \"message\": \"The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 3c4b3dc691296b6e6980815c4b48ddd2 in your message.)\",\n",
      "    \"type\": \"server_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "}\n",
      " 500 {'error': {'message': 'The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 3c4b3dc691296b6e6980815c4b48ddd2 in your message.)', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Thu, 01 Jun 2023 02:07:20 GMT', 'Content-Type': 'application/json', 'Content-Length': '366', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-organization': 'user-0zxqjsysyigfz9bgyzffde1h', 'openai-processing-ms': '31168', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '3000', 'x-ratelimit-remaining-requests': '2999', 'x-ratelimit-reset-requests': '20ms', 'x-request-id': '3c4b3dc691296b6e6980815c4b48ddd2', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '7d03e63c2ac54104-SIN', 'alt-svc': 'h3=\":443\"; ma=86400'}.\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain.llms import GPT4All\n",
    "from langchain import HuggingFaceHub, LLMChain\n",
    "\n",
    "# initialize Hub LLM\n",
    "# llm = HuggingFaceHub(repo_id='THUDM/chatglm-6b', model_kwargs={\"trust_remote_code\": True})\n",
    "# llm = HuggingFaceHub(repo_id=\"google/flan-t5-xl\", model_kwargs={\"temperature\":1e-10})\n",
    "# llm = ChatGLMService()\n",
    "# llm.load_model()\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "# embeddings = HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-xl\", \n",
    "#                                            model_kwargs={\"device\": \"cuda\"})\n",
    "persist_directory = 'rpmv2'\n",
    "txt_docsearch = Chroma.from_documents(txt_docs, embeddings, persist_directory=persist_directory)\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.5, max_tokens=3500)\n",
    "# llm = GPT4All(model=\"/home/jinzy/Downloads/ggml-gpt4all-j-v1.3-groovy.bin\", n_ctx=512, n_threads=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e2a3fb4-9f32-41c0-a406-85cbcf90f23d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80fa412d64534871be9011395688e224",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "txt_docsearch.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "659018cb-ee60-4d9f-8b90-eb5d4e9f94d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.5, max_tokens=3500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2322d6e-9d18-4b67-9a24-555ae2437215",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts.chat import (\n",
    "  ChatPromptTemplate,\n",
    "  SystemMessagePromptTemplate,\n",
    "  HumanMessagePromptTemplate\n",
    ")\n",
    "from langchain.memory import ConversationBufferMemory, ConversationSummaryBufferMemory\n",
    "\n",
    "\n",
    "rewritte_template = \"\"\"根据聊天记录和问题，把问题重写为一个独立的、完整的问题。\n",
    "聊天记录:\n",
    "{chat_history}\n",
    "问题: {question}\n",
    "独立问题:\"\"\"\n",
    "\n",
    "# 初始化 prompt 对象\n",
    "prompt = ChatPromptTemplate.from_template(rewritte_template)\n",
    "# 初始化问答链\n",
    "question_generator = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "refine_template = \"\"\"请检查下述检索结果是否可以用于更新回复内容。\n",
    "检索结果:\n",
    "{context_str}\n",
    "\n",
    "如果这个检索结果包含了有用的信息，请根据它和原始回复内容重新生成更好的回复。如果没有帮助，就返回原始回复内容。\n",
    "新的回复：\"\"\"\n",
    "# (\n",
    "#     \"We have the opportunity to refine the existing answer\"\n",
    "#     \"(only if needed) with some more context below.\\n\"\n",
    "#     \"------------\\n\"\n",
    "#     \"{context_str}\\n\"\n",
    "#     \"------------\\n\"\n",
    "#     \"Given the new context, refine the original answer to better \"\n",
    "#     \"answer the question. \"\n",
    "#     \"If the context isn't useful, return the original answer.\"\n",
    "# )\n",
    "refine_prompt = ChatPromptTemplate.from_template(refine_template)\n",
    "doc_chain = load_qa_chain(llm, chain_type=\"refine\", refine_prompt =refine_prompt)\n",
    "\n",
    "summary_template = \"\"\"根据聊天内容增量式生成摘要，在原始摘要上附加新的摘要。\n",
    "EXAMPLE\n",
    "当前摘要: \n",
    "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\n",
    "\n",
    "新的聊天内容:\n",
    "Human: Why do you think artificial intelligence is a force for good?\n",
    "AI: Because artificial intelligence will help humans reach their full potential.\n",
    "\n",
    "新的摘要:\n",
    "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\n",
    "END OF EXAMPLE\n",
    "\n",
    "当前摘要:\n",
    "{summary}\n",
    "\n",
    "新的聊天内容:\n",
    "{new_lines}\n",
    "\n",
    "新的摘要:\n",
    "\"\"\"\n",
    "\n",
    "summary_prompt = PromptTemplate(input_variables=[\"summary\", \"new_lines\"], template=summary_template)\n",
    "\n",
    "# memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, prompt = summary_prompt,\n",
    "        max_token_limit=1000, memory_key=\"chat_history\", return_messages=True)\n",
    "chain = ConversationalRetrievalChain(\n",
    "    retriever=txt_docsearch.as_retriever(),\n",
    "    question_generator=question_generator,\n",
    "    combine_docs_chain=doc_chain,\n",
    "    memory=memory\n",
    ")\n",
    "# chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=txt_docsearch.as_retriever(), \n",
    "#                                               chain_type='refine', memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a53bf8b-8559-4f77-8301-09b8190c7f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': '请用简单的中文，尽可能详细地解释一下RPM产品中的任务挖掘功能', 'chat_history': [HumanMessage(content='请用简单的语言尽可能详细地解释一下RPM产品中的任务挖掘功能', additional_kwargs={}, example=False), AIMessage(content='Robotic Process Mining (RPM) is a new family of techniques that aims to assist RPA developers and analysts in the early stages of the RPA lifecycle. RPM helps analysts draw a systematic inventory of tasks that can be automated with RPA and synthesizes executable specifications of such tasks, which can be used as a starting point.', additional_kwargs={}, example=False), HumanMessage(content='请给出一个RPM产品的实际案例', additional_kwargs={}, example=False), AIMessage(content='这篇论文所讨论的问题可以放在Robotic Process Mining (RPM) [2]的更广泛背景下来看。RPM是一系列的方法，用于发现员工在日常工作中执行的重复例程，并将这些例程转化为软件脚本以模拟它们的执行。RPM流程的第一步是记录一个或多个工人与一个或多个系统之间的交互。这些信息可以帮助我们更好地理解如何自动化这些例程，提高工作效率。', additional_kwargs={}, example=False), HumanMessage(content='请介绍一个RPM产品的实际案例', additional_kwargs={}, example=False), AIMessage(content='Robotic Process Mining (RPM) 是一种通过分析一个或多个工作人员与一个或多个软件应用程序之间的交互来发现可以使用 RPA 技术自动化的重复例程的技术。通常，RPM 技术的输入为用户交互日志（UI 日志）。这些 UI 日志是在执行业务流程中的一个或多个任务时记录的。', additional_kwargs={}, example=False), HumanMessage(content='请用简单的中文，尽可能详细地解释一下RPM产品中的任务挖掘功能', additional_kwargs={}, example=False), AIMessage(content='RPA tools can automate a wide range of routines, but it can be difficult to identify which routines in an organization would benefit from automation using RPA. A new class of tools called Robotic Process Mining (RPM) tools can help with this. RPM tools use techniques to analyze data collected during user-driven processes, which can help identify routines that would benefit from automation using RPA.', additional_kwargs={}, example=False), HumanMessage(content='请用简单的中文，尽可能详细地解释一下RPM产品中的任务挖掘功能', additional_kwargs={}, example=False), AIMessage(content='根据检索结果，我们可以进一步解释RPM是一种技术和工具类别，用于分析用户驱动任务执行期间收集的数据，以支持识别和评估自动化候选例程，并发现RPA机器人可以执行的例程规范。在这个背景下，用户驱动任务是指涉及用户（例如业务流程中的工作人员）与一个或多个对象之间的交互的任务。因此，RPM可以帮助企业自动化业务流程中的重复性任务，从而提高效率和生产力。', additional_kwargs={}, example=False)], 'answer': '根据检索结果，我们可以进一步解释RPM是一种技术和工具类别，用于分析用户驱动任务执行期间收集的数据，以支持识别和评估自动化候选例程，并发现RPA机器人可以执行的例程规范。在这个背景下，用户驱动任务是指涉及用户（例如业务流程中的工作人员）与一个或多个对象之间的交互的任务。因此，RPM可以帮助企业自动化业务流程中的重复性任务，从而提高效率和生产力。'}\n"
     ]
    }
   ],
   "source": [
    "query = \"请用简单的中文，尽可能详细地解释一下RPM产品中的任务挖掘功能\"\n",
    "result = chain({\"question\": query})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6df3305-135f-467e-a673-96190ca5eca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 8776afb7f82b9d963ba925e6bd78e18e in your message.).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这篇论文所讨论的问题可以放在Robotic Process Mining (RPM) [2]的更广泛背景下来看。RPM是一系列的方法，用于发现员工在日常工作中执行的重复例程，并将这些例程转化为软件脚本以模拟它们的执行。RPM流程的第一步是记录一个或多个工人与一个或多个系统之间的交互。这些信息可以帮助我们更好地理解如何自动化这些例程，提高工作效率。\n"
     ]
    }
   ],
   "source": [
    "query = \"请用简单的中文，给出一个RPM产品的实际案例\"\n",
    "#result = chain({\"question\": query, \"chat_history\": chat_history})\n",
    "result = chain({\"question\": query})\n",
    "#print(result)\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e59c0449-a976-4ab7-9b0a-a6e41757a833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robotic Process Mining (RPM) 是一种通过分析一个或多个工作人员与一个或多个软件应用程序之间的交互来发现可以使用 RPA 技术自动化的重复例程的技术。通常，RPM 技术的输入为用户交互日志（UI 日志）。这些 UI 日志是在执行业务流程中的一个或多个任务时记录的。\n"
     ]
    }
   ],
   "source": [
    "query = \"请用简单的中文，介绍一个RPM产品的实际案例\"\n",
    "#result = chain({\"question\": query, \"chat_history\": chat_history})\n",
    "result = chain({\"question\": query})\n",
    "\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08990737-6e20-4d05-9c6f-17d181b19b28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': '请介绍一个RPM产品的实际案例',\n",
       " 'chat_history': [HumanMessage(content='请用简单的语言尽可能详细地解释一下RPM产品中的任务挖掘功能', additional_kwargs={}, example=False),\n",
       "  AIMessage(content='Robotic Process Mining (RPM) is a new family of techniques that aims to assist RPA developers and analysts in the early stages of the RPA lifecycle. RPM helps analysts draw a systematic inventory of tasks that can be automated with RPA and synthesizes executable specifications of such tasks, which can be used as a starting point.', additional_kwargs={}, example=False),\n",
       "  HumanMessage(content='请给出一个RPM产品的实际案例', additional_kwargs={}, example=False),\n",
       "  AIMessage(content='这篇论文所讨论的问题可以放在Robotic Process Mining (RPM) [2]的更广泛背景下来看。RPM是一系列的方法，用于发现员工在日常工作中执行的重复例程，并将这些例程转化为软件脚本以模拟它们的执行。RPM流程的第一步是记录一个或多个工人与一个或多个系统之间的交互。这些信息可以帮助我们更好地理解如何自动化这些例程，提高工作效率。', additional_kwargs={}, example=False),\n",
       "  HumanMessage(content='请介绍一个RPM产品的实际案例', additional_kwargs={}, example=False),\n",
       "  AIMessage(content='Robotic Process Mining (RPM) 是一种通过分析一个或多个工作人员与一个或多个软件应用程序之间的交互来发现可以使用 RPA 技术自动化的重复例程的技术。通常，RPM 技术的输入为用户交互日志（UI 日志）。这些 UI 日志是在执行业务流程中的一个或多个任务时记录的。', additional_kwargs={}, example=False)],\n",
       " 'answer': 'Robotic Process Mining (RPM) 是一种通过分析一个或多个工作人员与一个或多个软件应用程序之间的交互来发现可以使用 RPA 技术自动化的重复例程的技术。通常，RPM 技术的输入为用户交互日志（UI 日志）。这些 UI 日志是在执行业务流程中的一个或多个任务时记录的。'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7be78c-252d-4c02-86d0-55ac3b30f270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"请介绍一个RPM产品的实际案例\"\n",
    "# #result = chain({\"question\": query, \"chat_history\": chat_history})\n",
    "# result = chain({\"question\": query + ' 请用中文回答。'})\n",
    "\n",
    "# print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce281092-800b-4fc1-94b9-1c742d195d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d87a49-ec8a-4c43-933a-3ed3a9458b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain.memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afd39d9-f77c-488a-9ef5-ad908a250f4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f38d1f8-366c-42aa-a8a8-db6bd59d4af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import List, Optional\n",
    "# from langchain.llms.base import LLM\n",
    "# from langchain.llms.utils import enforce_stop_tokens\n",
    "# from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "\n",
    "# class ChatGLMService(LLM):\n",
    "#     max_token: int = 10000\n",
    "#     temperature: float = 0.1\n",
    "#     top_p = 0.9\n",
    "#     history = []\n",
    "#     tokenizer: object = None\n",
    "#     model: object = None\n",
    "\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "\n",
    "#     @property\n",
    "#     def _llm_type(self) -> str:\n",
    "#         return \"ChatGLM\"\n",
    "\n",
    "#     def _call(self,\n",
    "#               prompt: str,\n",
    "#               stop: Optional[List[str]] = None) -> str:\n",
    "#         response, _ = self.model.chat(\n",
    "#             self.tokenizer,\n",
    "#             prompt,\n",
    "#             history=self.history,\n",
    "#             max_length=self.max_token,\n",
    "#             temperature=self.temperature,\n",
    "#         )\n",
    "#         if stop is not None:\n",
    "#             response = enforce_stop_tokens(response, stop)\n",
    "#         self.history = self.history + [[None, response]]\n",
    "#         return response\n",
    "\n",
    "#     def load_model(self, model_name_or_path: str = \"THUDM/chatglm-6b\"):\n",
    "#         self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "#             model_name_or_path,\n",
    "#             trust_remote_code=True\n",
    "#         )\n",
    "#         print('加载chatglm-6b', model_name_or_path)\n",
    "#         # 这里使用的是 量化后的模型int4\n",
    "#         self.model = AutoModel.from_pretrained(model_name_or_path, trust_remote_code=True).float().half().cuda()\n",
    "#         #.half().quantize(4).cuda()\n",
    "#         #.half().cuda()\n",
    "#         self.model=self.model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68148e9b-cbf3-474c-8ccf-68b121d023fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import chromadb\n",
    "# import chromadb.config\n",
    "\n",
    "# persist_directory = 'rpmv2'\n",
    "# txt_docsearch._client_settings = chromadb.config.Settings(\n",
    "#                     chroma_db_impl=\"duckdb+parquet\", persist_directory = persist_directory\n",
    "#                 )\n",
    "# txt_docsearch._client = chromadb.Client(txt_docsearch._client_settings)\n",
    "# txt_docsearch._persist_directory = persist_directory\n",
    "# txt_docsearch.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7029d8-ce09-499a-9c79-841a9d2c9d28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df46859b-dfa1-4b57-8a47-c03ad2a3513f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
