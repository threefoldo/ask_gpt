{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "{}\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b7f80b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://chatgpt.snaplearn.ai/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "print(os.environ.get(\"OPENAI_API_HOST\"))\n",
    "\n",
    "# openai.api_base = os.environ.get(\"OPENAI_API_HOST\")\n",
    "#openai.api_base = os.environ.get(\"OPENAI_API_HOST\") + '/v1/chat'\n",
    "#openai.api_base = \"https://api.openai.com/v1\"\n",
    "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "# text-embedding-ada-002 most likely\n",
    "embedding_model = os.getenv(\"EMBED_MODEL\", \"text-embedding-ada-002\")\n",
    "text_model = os.getenv(\"TEXT_MODEL\", \"text-davinci-003\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd9e3f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We like to wrap our LLM calls to langchain models, to have a more generic interface\n",
    "# from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "# from langchain.text_splitter import TokenTextSplitter\n",
    "# from langchain.llms import OpenAI\n",
    "\n",
    "# # We're using the new name llama_index, but you can find lots of example with the old name\n",
    "# # gpt_index as well\n",
    "# from llama_index import download_loader\n",
    "# from llama_index.node_parser import SimpleNodeParser\n",
    "# from llama_index import LLMPredictor, ServiceContext, PromptHelper, LangchainEmbedding\n",
    "\n",
    "\n",
    "# chunk_len = 256\n",
    "# chunk_overlap = 32\n",
    "\n",
    "# embed_model = LangchainEmbedding(OpenAIEmbeddings(model=embedding_model))\n",
    "# llm = OpenAI(model_name=text_model, max_tokens=2000, temperature=0)\n",
    "# llm_predictor = LLMPredictor(llm=llm)\n",
    "# # Llama-index parameterization\n",
    "# splitter = TokenTextSplitter(chunk_size=chunk_len, chunk_overlap=chunk_overlap)\n",
    "# node_parser = SimpleNodeParser(\n",
    "#     text_splitter=splitter, include_extra_info=True, include_prev_next_rel=False\n",
    "# )\n",
    "# prompt_helper = PromptHelper.from_llm_predictor(\n",
    "#     llm_predictor=llm_predictor,\n",
    "# )\n",
    "# service_context = ServiceContext.from_defaults(\n",
    "#     llm_predictor=llm_predictor,\n",
    "#     prompt_helper=prompt_helper,\n",
    "#     embed_model=embed_model,\n",
    "#     node_parser=node_parser,\n",
    "#     # chunk_size_limit=chunk_len,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76b8cf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from llama_index import SimpleDirectoryReader, GPTVectorStoreIndex\n",
    "from llama_index import OpenAIEmbedding, ServiceContext\n",
    "from llama_index.node_parser import SimpleNodeParser\n",
    "from llama_index.langchain_helpers.text_splitter import TokenTextSplitter\n",
    "\n",
    "chunk_len = 256\n",
    "chunk_overlap = 32\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "tokenizer = lambda text: enc.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "embed_model = OpenAIEmbedding()\n",
    "embed_model._tokenizer = tokenizer\n",
    "node_parser = SimpleNodeParser(text_splitter=TokenTextSplitter(tokenizer=tokenizer,chunk_size=chunk_len, chunk_overlap=chunk_overlap))\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    embed_model=embed_model, node_parser=node_parser\n",
    ")\n",
    "\n",
    "documents = SimpleDirectoryReader('/home/jinzy/work/pcg/chatdocs', recursive=True, required_exts=['.md', '.py']).load_data()\n",
    "index = GPTVectorStoreIndex.from_documents(documents, service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "970fa62e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1335"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2317ac23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(response='\\nThere is no project specifically about chatting with multiple documents. However, it is possible to upload multiple documents and have a conversation with them using the ChatFiles project.', source_nodes=[NodeWithScore(node=Node(text='\\n\\nChatFiles\\n', doc_id='d96f82a7-87a7-40af-911d-7c967b15b384', embedding=None, doc_hash='ffd6d21fa449ba78f0cf92228f7d69f88fa449ceb139cf142be603b9b4bf86ef', extra_info=None, node_info={'start': 0, 'end': 12}, relationships={<DocumentRelationship.SOURCE: '1'>: '96b0593e-7b80-4def-8cb1-d60b1e73c9d3'}), score=0.8276551474399388), NodeWithScore(node=Node(text='\\n\\nchat with file\\n1. upload a file.\\n2. have a conversation with it.\\n\\n', doc_id='15dab744-9171-4133-8e95-b282d5e489a6', embedding=None, doc_hash='6741cb94b7aea16b8c4bc7b58da69511ad0d23cc444786b4bbaf7afcdeefcaa5', extra_info=None, node_info={'start': 0, 'end': 68}, relationships={<DocumentRelationship.SOURCE: '1'>: 'd895c924-5c2c-4b6f-b2ab-4877cd0c49ec'}), score=0.8235879317975959)], extra_info={'d96f82a7-87a7-40af-911d-7c967b15b384': None, '15dab744-9171-4133-8e95-b282d5e489a6': None})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "query_engine.query('which project is about chatting with multiple documnts?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f223dc37-59aa-4321-aadb-2fd47c7f31b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "index.storage_context.persist(persist_dir=\"chatwithdocs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "585b75d1-5225-42df-86d6-95bb06980a3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(response='\\nThe ChatFiles project is a repository that allows users to upload a file and have a conversation with it. It is based on the jerryjliu/llama_index library and is inspired by the mckaywrigley/chatbot-ui and madawei2699/myGPTReader libraries. The main modules of the project include a file uploader, a conversation engine, and a user interface.', source_nodes=[NodeWithScore(node=Node(text='\\n\\nChatFiles\\n', doc_id='d96f82a7-87a7-40af-911d-7c967b15b384', embedding=None, doc_hash='ffd6d21fa449ba78f0cf92228f7d69f88fa449ceb139cf142be603b9b4bf86ef', extra_info=None, node_info={'start': 0, 'end': 12}, relationships={<DocumentRelationship.SOURCE: '1'>: '96b0593e-7b80-4def-8cb1-d60b1e73c9d3'}), score=0.8301731240272862), NodeWithScore(node=Node(text='\\n\\nChatFiles\\n\\nEN | 中文文档\\n\\n> this repository use jerryjliu/llama_index, based on mckaywrigley/chatbot-ui, inspired by madawei2699/myGPTReader\\n\\n!Chatfiles\\n\\n**Upload your file and have a conversation with it.**\\n\\n', doc_id='5e87920b-0797-4f69-9ebb-8db1b3302f37', embedding=None, doc_hash='e64951d9fa99fe509a1928cc1ed6430674a3bd9dc995b05dc525219cfe7a85df', extra_info=None, node_info={'start': 0, 'end': 207}, relationships={<DocumentRelationship.SOURCE: '1'>: '2a853fe3-f46c-451e-b4fa-97657fd47f76'}), score=0.8278060544595881)], extra_info={'d96f82a7-87a7-40af-911d-7c967b15b384': None, '5e87920b-0797-4f69-9ebb-8db1b3302f37': None})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_engine.query('please tell me how the chatfiles project works? what is its main modules and what libraries it uses?')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
